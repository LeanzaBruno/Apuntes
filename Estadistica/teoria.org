#+TITLE: Teoría completa y resumida de Estadística
#+begin_export latex
\clearpage
\tableofcontents
\clearpage
#+end_export

* Probabilidad

#+begin_export latex
\clearpage
#+end_export
* Variables aleatorias

** Variables aleatorias unidimensionales

** Definición

Una variable aleatoria es aquella función definida en el espacio muestral que asocia a cada punto muestral un número perteneciente a los reales. Al realizar el experimento aleatorio, la variable toma un solo valor. 

** Clasificación
Las variables aleatorias se clasifican en discretas y continuas. Las primeras solo pueden tomar números enteros y la segunda cualquier número real.

** Distribución de Probabilidades
Existe una asignación de probabilidades a cada uno de los posibles valroes que puede tomar una variable aleatoria. Esa correspondencia entre los valores de la variable aleatoria y su probabilidad de ocurrencia es lo que se denomina b **distribución de probabilidades**.
  
-------------------------------------------------------------------------------

** Variable aleatoria discreta

*** Función de masa de probabilidad o función de cuantía
Esta función es la forma matemática de expresar la correspondencia entre *los valores de la variable y sus probabilidades*.

$$f(x) = P(X=a)$$

Esto se lee *"la probabilidad de que la variable aleatoria* $X$ *tome el valor a*".

Para satisfacer los axiomas de la teoría de la probabilidad y ser realmente una función de probabilidad deberá cumplir con los siguientes requisitos:

1. $f(x_i)$ debe tener un valor númerico para todos los posibles valores de la variable aleatoria.

2. $0 \leq f(x_i) \leq 1, \forall x$.

3. $\sum f(x_i) = 1, \forall x$.

*** Función acumulativa o de distribución
Esta función $$F(x)$$ (no confundir con $$f(x)$$) es simplemente *la probabilidad de que la variable aleatoria tome valores menores o iguales que un valor dado*. 

$$F(z) = P(X \leq z)$$

Para el caso de una variable discreta esta función resulta de la suma de los valores de la función de masa de probabilidad evaluada en aquellos valores menores o iguales que z: 

$$F(x) = P(X \leq z) = \sum_{\forall x_i \leq z} f(x_i)$$

La gráfica de esta función es de tipo *escalonada*.


-------------------------------------------------------------------------------

** Variable aleatoria continua

*** Función de densidad o de masa de probabilidad
No podemos utilizar el mismo concepto de función de cuantía para variables aleatorias continuas ya que su dominio es infinito. Por lo que el problema de especificar la distribución de probabilidades para una variable aleatoria continua se deriva de realizar el siguiente razonamiento.
Si el eje x se divide en un gran número de intervalos infinitesimales, cada uno de longitud $$dx$$, es posible definir una función f(x) tal que la probabilidad de que $$x$$ esté en un intervalo $$(x, x+dx$$ es $$f(x)dx$$.
Ya que ocurrencias en distintos intervalos son eventos mutuamente excluyentes, se deduce que la probabilidad de que una variable aleatoria tome valores en un tervalo de longitud finita, es la suma de probabilidades en los intervalos infinitesimales, es decir, $$\int f(x)dx$$ sobre el intervalo de interés. 
**De esta manera, el área bajo la curva de esta función en un intervalo, representa la probabilidad de que la variable tome valores en el intervalo**: 

$$P( x_i \leq x \leq x_2 ) = \int_{x_1}^{x_2} f(x)dx$$

La probabilidad de que una variable aleatoria continua tome un valor específico es 0, dado que 

$$P(X=z) = \int_{z}^{z} f(x)dx = 0$$

Por ende, 

$$P(a \leq X \leq B ) = P (a < X B ) = P(a \leq X < B) = P(a < X \leq B)$$

	El valor de f(x) en sí mismo no es una probabilidad, sino solamente la medida de la densidad de probabilidad en un intervalo.

Al igual que la función de cuantía para variables discretas, la función de densidad debe cumplir ciertas condiciones para poder ser una *función de probabilidad*.

1. $$f(x) \geq 0$$.

2. $$\smallint_{-\infty}^{\infty} f(x)dx = 1$$.

*** Función de distribución o de acumulación
Se defina la función de distribución acumulativa de la v.a. X, $$F(x)$$ (*no confundir con f(x), la función de distribución usa una F grande*), como la probabilidad de que X tome valores menos o iguales a un valor z:

$$F(z) = P(X \leq z) = \int_{-\infty}^z f(x)x$$

**** Propiedades de la función de distribución

1. $$F(-\infty) = \lim_{z \to -\infty} \int_{-\infty}^z f(x)dx = 0$$

2. $$F(\infty) = \lim_{z \to \infty} \int_{-\infty}^z f(x)dx = 1$$

3. Es una función monótona creciente. 

4. Se puede hallar la función de densidad derivando la función de distribución y viceversa, aplicando una integral indefinida a la función de densidad se halla la función de distribución.



-------------------------------------------------------------------------------

** Variables aleatorias bidimensionales

** Variables aleatorias discretas

*** Función de cuantía conjunta

$$f(x,y) = P(X=x;Y=y)$$

**** Función de cuantía marginales

$$f(x) = P(X=x) = \sum_{\forall y_i}^n f(x,y_i), i \in \{0, 1, ..., n\}$$

$$f(y) = P(Y=y) = \sum_{\forall x_i}^n f(x_i, y), i \in \{0, 1, ..., n\}$$

*** Función acumulativa conjunta


$$F(x,y) = P(X \leq x ; Y \leq y) = \sum_{x_i}^n \sum_{y_j}^m f(x_i, y_j), i \in \{0, 1, ..., n\},\ j \in \{0, 1, ..., m\}$$

**** Funciones acumulativas marginales

$$F(x) = P(X \leq z ) = \sum_{x_i \leq x} f(x_i) = \sum_{\forall x_i \leq x} \sum_{y_j = 0}^n f(x_i, y_j)$$

$$F(y) = P(Y \leq y) = \sum_{y_j \leq y} f(y_j) = \sum_{x_i = 0}^m \sum_{y_j \leq y} f(x_i, y_j)$$


*** Funciones condicionales
$$
f(x/y)=P(X = x / Y = y_0) = \frac{P(X=x \land Y = y)} {P(Y = y_0)} = \frac{f(x,y)}{\sum_i^n f(x_i, y)} = \frac{f(x,y)}{f(y)}
$$

Es inverso cuando al condición está dada por X.

*** Independencia de variables aleatorias
Si las variables son independientes entonces se cumple que:

\begin{align*}
P(X = x_0, Y = y_0 ) &= P(X = x_0) \cdot P(Y = y_0)\\
f(x_0, y_0) &= f(x_0) \cdot f(y_0)
\end{align*}

** Variables aleatorias continuas

*** Función de densidad conjunta

$$P( x_i \leq X \leq x_2 ; y_i \leq Y \leq y_2) = \int_{x_1}^{x_2} \int_{y_1}^{y_2} f(x,y)dxdy$$

Graficamente, esto representa el volumen debajo de la función f(x,y) sobre la región dada.

*** Funciones de densidad marginales

$$
f(x) &= \int_{-\infty}^{\infty} f(x,y)dy
f(y) &= \int_{-\infty}^{\infty} f(x,y)dx
$$

*** Función de distribución conjunta
$$
F(x,y) = P(-\infty \leq X \leq x_1 ; -\infty \leq Y \leq y_1)
= \int_{-\infty}^{x_1} \int_{-\infty}^{y_1} f(x,y)dxdy
$$

Las propiedades son análogas al caso unidimensional

*** Funciones de distribución marginales

\begin{align*}
F(x) = P(X \leq t) = \int_{-\infty}^{t} \int_{-\infty}^{\infty} f(x,y)dydx
       = \int_{-\infty}^{t} f(x)dx
\end{align*}

*** Función de distribución \to Función de densidad

*** Función de densidad \to Función de distribución
*** Función de probabilidad condicional

- Función de distribución condicionada de la v.a. X dado $Y=y_0$:

  $$
  F(x/y) = \int_{-\infty}^x f(x/y)dx = \frac{\int_{-\infty}^x f(x/y)dx} {f(y)}
  $$
  
*** Independencia de variables aleatorias
Si dos variables aleatorias distribuidas conjuntamente son independientes, entonces:

\begin{align*}
P(X \leq x_0 ; Y \leq y_0) &= P(X \leq x_0) \cdot P(Y \leq y_0)\\
F(x_0,y_0) &= F(x_0) \cdot F(y_0)
\end{align*}


#+begin_export latex
\clearpage
#+end_export
* Características
** Medidas de tendencia central

** Promedios
*** Esperanza matemática
Se define como

\begin{align}
E(X) &= \sum_{i}^{n} x_i f(x_i) &&\text{Caso discreto}\\
E(X) &= \int_{-\infty}^{\infty} x f(x) dx &&\text{Caso continuo}
\end{align}

La esperanza matemática representa el ~centro de masa~ de la función
de probabilidad.

----------
4
** Medidas de ubicación
*** Mediana
Representa el valor de la variable aleatoria que divide a la mitad a
todos los valores posibles que puede tomar y cumple la siguiente
condición:

\begin{align}
P(X \leq Mediana) = 0,5 &= \sum_i f(x_i) && \text{Caso discreto}\\
                  &= \int_{-\infty}^{Mediana} f(x) dx && \text{Caso continuo}
\end{align}

*** Modo
El modo es el valor de la variable que ocurre con más frecuencia. En
el caso discreto, observando su tabla de probabilidades puede verse
rapidamente que el valor cuya probabilidad sea la mayor, será el
modo.
Para el caso continuo, debemos recurrir a las derivadas de la función
de densidad dado que en realidad la función de densidad en sí misma no
representa la probabilidad de ocurrencia la variable. Entonces, el
valor que sea el modo de la variable aleatoria cumplirá con

\begin{align}
\frac{d}{dx}f(x) = 0\quad\quad y\quad\quad \frac{d^2}{dx^2} f(x) < 0
\end{align}


*** Cuantiles
Se denomina cuantil de orden p al valor de la variable que cumple con
la siguiente condición

\begin{align}
P(X\leq x_p) = p \quad = \quad P(X > x_p)=1-p
\end{align}

**** ~Cuartiles~:
Existen 3 cuartiles y dividen la distribución de probabilidades en
cuatro partes.

\begin{align}
P(X\leq x_p) = \frac{i}{4},\ i=1,2,3.
\end{align}

El cuartil es orden 2 corresponde a la mediana.

**** ~Deciles~:
Dividen la distribución de probabilidades en diez partes, por lo tanto
hay 9 deciles.

\begin{align}
P(X\leq x_p) = \frac{i}{10},\ i=1,2,...,9.
\end{align}

**** ~Percentiles~:
Dividen la distribución de probabilidades en 100 partes, por tanto hay
99 percentiles. Son de interés cuando se desea analizar detalladamente
la distribución de probabilidades

\begin{align}
P(X\leq x_p) = \frac{i}{99},\ i=1,2,...,99.
\end{align}

#+begin_export latex
\clearpage
#+end_export

** Momento
Se los define como los promedios de distintas potencias de la variable
aleatoria

*** Momentos centrados respecto al valor medio \epsilon_k
Elijo la letra \epsilon en vez de \mu para este momento centrado respecto a la media porque su forma me recuerda que está centrado respecto a la esperanza.

\begin{align}
\epsilon_k = E[(X-E[X])^k] &= \sum_i (x_i-E[X])^k \cdot f(x_i) && \text{Caso discreto}\\
                  &= \int_{-\infty}^{\infty} (x_i-E[X])x^k \cdot f(x)dx && \text{Caso continuo}
\end{align}

*** Momentos ~NO~ centrados o centrados al origen \theta_k
Estos momentos se obtienen con respecto al origen.
Similar a lo anterior, este símbolo me recuerda que el momento está centrado respecto al origen, o dicho de otra forma, es un momento ~no centrado~.

\begin{align}
\theta_k = E[X^k] &= \sum_i (x_i-0)^k \cdot f(x_i) \nonumber \\
                  &= \sum_i x_i^k \cdot f(x_i) && \text{Caso discreto}\\
                  &= \int_{-\infty}^{\infty} (x-0)^k \cdot f(x)dx \nonumber \\
                  &= \int_{-\infty}^{\infty} x^k \cdot f(x)dx && \text{Caso continuo}
\end{align}


#+begin_export latex
\clearpage
#+end_export
** Medidas de variabilidad
** Rango
Es simplemente la diferencia entre el mayor y el menor valor de la
variable.

** Varianza
Es la medida de dispersión más usada y es el momento de orden 2. Nuevamente hago un cambio de simbología

\begin{align}
\epsilon_2 = Var(X) =
\left\{
       \begin{array}{ll}
                    \sum_i [x_i-E(x)]^2 f(x_i)                & \text{Caso discreto}\\\\
                    \int_{-\infty}^{\infty} [x-E(x)]^2 f(x)dx & \text{Caso continuo}
       \end{array}
\right.
\end{align}


La primera expresión puede simplificarse y ser expresada en función de
los momentos con respecto al origen(no centrados) al desarrollar la
potencia del binomio, resultando:

\begin{align}
\epsilon_2 = Var[X] = E[X^2]-(E[X])^2 = \theta_2 - \theta_1^2
\end{align}


** Desvío estándar \sigma
Para algunas aplicaciones suele ser más conveniente utilizar una
medida de variabilidad en las mismas unidades de la variable, para eso
existe el desvío estándar como la raíz cuadrada positiva de la
varianza:

\begin{align}
\sigma = + \sqrt{Var[X]} = + \sqrt{\theta_2 - \theta_1^2}
\end{align}

*** Coeficiente de variabilidad
Es un coeficiente adimensional que se obtiene de dividir el desvío por
el valor esperado. Se lo utilza para comparar las dispersiones de
poblaciones que corresponden a variables _con diferentes unidades_ o
bien en aquellos casos en que tienen mismas unidades pero sus valores
medios son muy diferentes.

\begin{align}
C_v = \frac{\sigma[X]}{E[X]}
\end{align}

#+begin_export latex
\clearpage
#+end_export

** Medidas de asimetría
De igual forma que la media y la varianza miden la ubicación y
dispersión, los momentos más altos miden otras propiedades.

** Momento ~centrado~ de orden 3 \epsilon_3
El tercer momento respecto a la media es usado para determinar si una
distribución es simétrica o asimétrica. Si

\begin{align*}
\epsilon_3 &= 3 \Rightarrow \text{simétrica}\\
\epsilon_3 &< 3 \Rightarrow \text{asimétrica a la derecha}\\
\epsilon_3 &> 3 \Rightarrow \text{asimétrica a la izquierda}
\end{align*}

** Coeficiente de asimetría
El momento \epsilon_3 en si mismo no es una buena medida de asimetría ya
que tiene las mismas unidades que la variable, por eso se define una
medida relativa denominada ~coeficiente de asimetría~:

\begin{align}
\gamma = \frac{\epsilon_3}{\sigma^3} = \frac{\theta_3 -3\theta_2 \theta + 2\theta^3}{(\theta_2-\theta^2)^{3/2}}
\end{align}


#+begin_export latex
\clearpage
#+end_export

** Medidas de curtosis
Una cuarta propiedad de las variables aleatorias se basa en el momento
~centrado~ de cuarto orden \epsilon_4, que permite evaluar el empinamiento
o aplastamiento de la distribución de probabilidades comparada con una
curva tomada como modelo(la curva normal).

\begin{align}
\gamma_2 = \frac{\epsilon_4}{\sigma^4} = \frac{\theta_4-4\theta_3 \theta+6\theta_2 \theta^2-3\theta^4}{\sigma^4}
\end{align}

La curtosis de la curva normal es igual a 3, y se la denomina
distribución *mesocúrtica*. Para distribuciones que presenten mayor
concentración de probabilidad cerca de la media, mayor que en la
Normal, la curtosis será ~mayor que 3~ y se denominará
*leptocúrtica*. En caso que la concentración alrededor de la media sea
menor que en la Normal, la curtosis será ~menor que 3~ y se llamará
*platicúrtica*.

#+CAPTION: Demostración de la curtosis
#+NAME: figura
[[./curtosis.png]]

#+begin_export latex
\clearpage
#+end_export

** Momentos bidimensionales

** Momentos ~NO~ centrados

\begin{align}
\theta_{l,n} =
\left\{
       \begin{array}{ll}
          \sum_i \sum_j x_i^l y_j^n f(x,y) & \mbox{Caso discreto}\\\\
          \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^l y^n f(x,y) dxdy & \mbox{Caso continuo}
       \end{array}
\right.
\end{align}

*** Esperanzas marginales


\begin{align}
E(X)=\theta_{1,0}=
\left\{
       \begin{array}{ll}
         \sum_i \sum_j x_i^l  y_j^0  f(x,y) & \mbox{Caso discreto}\\\\
         \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^l  y^0  f(x,y) dydx & \mbox{Caso continuo}
       \end{array}
\right.
\end{align}


\begin{align}
E(Y)=\theta_{0,1}=
\left\{
       \begin{array}{ll}
         \sum_i \sum_j x_i^0  y_j^n  f(x,y) & \mbox{Caso discreto}\\\\
         \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^0  y^n  f(x,y) dxdy & \mbox{Caso continuo}
       \end{array}
\right.
\end{align}



#+begin_export latex
\clearpage
#+end_export

** Momentos centrados respecto a valores medios

\begin{align}
\epsilon_{e,s} = E[X-E(X)]^e \cdot E[Y - E(Y)]^s
\end{align}



#+begin_export latex
\clearpage
#+end_export

* Modelos estadísticos
La aplicación de la teoría de probabilidades en situaciones concretas, cuando se presentan determinadas características particulares, ha originado una serie de modelos. Estos modelos son funciones de probabilidad, por lo que cada uno deberá cumplir con todas las propiedades de una función de probabilidad. También poseen sus propias característica que serán determinadas en cada modelo.

** Modelos de variables discretas

*** Modelo de Bernoulli
Tal vez la situación más común que se presenta es aquella en la que los resultados de los experimentos pueden separarse en dos categorías /mutuamente excluyentes/: éxito o fraccaso.

Puede entonces decirse entonces que si la variable aleatoria X es x=0 \rightarrow fracaso, y si x=1 \rightarrow éxito.

**** *Función de cuantía*

\begin{align*}
f(x)=
\left\{
\begin{array}{ll}
   p & \mbox{si } x=1\\
   1-p & \mbox{si } x=0
\end{array}
\right.
\end{align*}

donde /p/ es la probabilidad de éxito.

**** *Esperanza*
$$
E(x)=p
$$

**** *Varianza*
$$
V(x) = p(1-p)
$$



------------------------------
*** Modelo Binomial
Si se realizan una serie de pruebas de tipo Bernoulli cuyos resultados sean /independientes/ y si la probabilidad de éxito permanece /invariable/ en todas ellas, se origina el modelo binomial. Los parámetros de este modelo son /n/, el número de pruebas, y /p/ la probabilidad de éxito.

**** *Función de cuantía*

\begin{align*}
P(X=x) = f(x) &= \binom{n}{x} p^x (1-p)^{n-x}\\
              &= \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}
\end{align*}

El modelo se denomina binomial porque puede considerarse como el desarrollo del binomio $(p+q)^n$.

**** Esperanza

$$
E(X)= \sum_{i=0}^n x_i f(x_i) = np
$$

**** Varianza
$$
Var(X)=n p q
$$


------------------------------
*** Modelo de Poisson
Generalmente este modelo se vincula a aquellos eventos que ocurren en una unidad de /tiempo/, luego el período de tiempo en el que se realiza el análisis constituye una secuencia de pruebas independientes cada una con distribución binomial.

Si se tomara para el análisis un intervalo de tiempo igual al doble o al triple del inicial se verá que el parámetro es también igual al doboe, al triple, etc. Marcando esto la dependencia del tiempo de este modelo y por ello vinculado a los procesos *estocásticos*, esto es, procesos en los que interesa la secuencia del tiempo.

Considerando las condiciones que caracterizan al modelo binomial, es necesario analizar que sucede con la distribución de la variable si el número de pruebas /n/ se incrementa y asociado a esto, la probabilidad /p/ disminuye. Es decir, mientras n aumenta y p disminuye, el número promedio de eventos en el intervalo total debe permanecer constante e igual a *n \cdot p*. Denominando a esta constante \lambda y considerando la función de masa de probabilidad de x cuando n\rightarrow\infty \land p \rightarrow 0 y llamando a \lambda =n \cdot p, a partir del modelo binomial, se obtiene:

**** *Función de cuantía*
$$
P(X=x)=f(x)=\frac{e^{-\lambda} \lambda^x}{x!}
$$

donde \lambda es el parámetro del modelo.

**** Esperanza
$$
E(X)=\lambda
$$

**** Varianza
$$
Var(X)= \lambda
$$


------------------------------
*** Modelo hipergeométrico
Este modelo surge cuando se realiza un muestreo sin reposición de una población finita con sus elementos clasificados en dos categorías.

Si N es el total de elementos de los cuales hay k de una categoría y N-k de otra, al realizar una extracción de n elementos, sin reposición, cada extracción que se realice posteriormente es dependiente del resultado de la extracción anterior con lo cual va cambiando la probabilidad del éxito.

Para derivar la función correspondiente a la variable aleatoria X: número de éxitos o elementos pertenecientes a la categoría que se estudia, en una extracción, se deberán considerar todas las maneras posibles o combinaciones de extraer x elementos de la categoría deseada, de los n extraídos y los restantes que pertenezcan a la otra categoría. El total de casos se obtiene de las combinaciones del total N extraídos de a n. Luego la función de masa de probabilidad es

**** *Función de masa de probabilidad*
$$
P(X=x)=f(x)= \frac{ \binom{k}{n} \binom{N-k}{n-x} }{\binom{N}{n}}
$$

**** Esperanza
$$
E(X)=np = n \frac{k}{N}
$$

**** Varianza
$$
Var(X) = npq \frac{N-n}{N-1}
$$

siendo $\frac{N-n}{N-1}$ el factor de correción por muestreo sin reposición y población finita.
Cuando $\frac{n}{N} \leq 0,05$ la distribución hipergeométrica \rightarrow binomial.



#+begin_export latex
\clearpage
#+end_export
** Modelos para variables continuas
*** Modelo exponencial
Este modelo surge al considerar el *tiempo hasta* la primera ocurrencia de un evento que pueda ser considerado como proceso de Poisson.
Si la variable aleatoria es ahora el tiempo transcurrido hasta que se verifica la primera ocurrencia, entonces será una /varible continua/. La probabilidad que T exceda algún valor t es lo mismo que decir que *no* se verificaron ocurrencias en ese intervalo de longitud, lo que es equivalente a decir que la variable aleatoria *N° de ocurrencias* de tipo Poisson toma el valor 0.

\begin{align*}
P(T>t) &= P(X=0) \\
P(X=0 = \frac{ e^{-lambda} \lambda^0} {0!}= e^{\lambda}
\end{align*}

**** Función de distribución
Esto permite obtener la función de distribución para la variable continua de tiempo T:

\begin{align}
F(t) = P(T \leq t) = 1-P(T>t)
F(t) = 1-e^{-\lambdat} \quad \mbox{Función de distribución}
\end{align}

La función de densidad se obtiene derivando la expresión anterior:
**** Función de densidad
\begin{align}
f(t) = \lambda e^{-\lambda t}
\end{align}

Cumpliendo con las propiedades de estacionariedad e independencia de los procesos de Poisson, e^{\lambdat} da la probabilidad de que no se verifiquen eventos en algún intervalo de tiempo de longitud t, estando o no en el origen en el tiempo 0.

**** Esperanza
$$
E(T)= \frac{1}{\lambda}
$$

****
$$
Var(T) = \frac{2}{\lambda^2}
$$

Recordar que \lambda representa el número promedio de *ocurrencias en los procesos* Poisson, aquí 1/\lambda representa el tiempo *promedio entre ocurrencias*

Una característica de los procesos de Poisson es que no tienen memoria. Esto significa que el comportamiento futuro es independiente de lo registrado en el presente o pasado. Para demostrarlo puede calcularse la probabilidad condicional de T dado que T>t_0, osea la probabilidad condicional del tiempo entre ocurrencias dado que no se han registrado ocurrencias antes del tiempo t_o; llegandose al final del cálculo a la misma expresión del modelo.


